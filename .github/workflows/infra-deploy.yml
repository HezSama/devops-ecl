name: Provision Infrastructure by Terraform
on:
  workflow_dispatch:
    inputs:
      env:
        description: 'Environment to deploy to'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - prod
    
env:
  ENV: ${{ github.event.inputs.env }}
    
   
    
permissions:
  contents: read
  id-token: write 


jobs:
  terraform:
    name: Provision Infrastructure by Terraform
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: app.tf

    steps:
      - name: Checkout 
        uses: actions/checkout@v4

      - name: Configuration the AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
      
        with:
          role-to-assume: ${{ secrets.aws_rta }}
          aws-region: ${{ secrets.AWS_REGION }}
          role-session-name: cuongnvecl-github-svc-tf-provision-infra

      - name: list the directory failed
        run: |
          ls -la -R /home/runner/work/devops-ecl/devops-ecl/app.tf

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          cli_version: v1.11.3

      - name: Terraform Init
        run: terraform init -backend-config="key=github_ci/infra/${{ github.ref_name }}-provisionecl1.tfstate -reconfigure"
        


      - name: Terraform Plan depends on env
        run: |
          terraform workspace select ${{ github.event.inputs.env }} || terraform workspace new ${{ github.event.inputs.env }}
          terraform plan --var-file=envs/${{ github.event.inputs.env }}.tfvars

      - name: Terraform Apply depends on env
        run: |
          terraform apply -auto-approve --var-file=envs/${{ github.event.inputs.env }}.tfvars

  init-kubeconfig:
    name: Init kubeconfig
    needs: terraform
    runs-on: ubuntu-latest

    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      EKS_CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME }}

    steps: 
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: ${{ secrets.aws_rta }}
        aws-region: ${{ secrets.AWS_REGION }}
        role-session-name: cuongnvecl-github-svc-tf-provision-infra
    - name: Get AWS user identity
      id: identity
      run: |
        AWS_USER_ARN=$(aws sts get-caller-identity --profile default | jq -r '.Arn')
        AWS_USERNAME=$(echo "$AWS_USER_ARN" | sed 's|.*/||')
        TYPE_ACCESS=cluster
        POLICY_ARN=arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy

        echo "Configurations"
        echo "\n AWS_USER_ARN = $AWS_USER_ARN \
        \n AWS_USERNAME = $AWS_USERNAME \
        \n CLUSTER_NAME = $EKS_CLUSTER_NAME \
        \n AWS_PROFILE = $awsprofile"

    - name: Grant access  
      run: 
        aws eks create-access-entry --cluster-name $EKS_CLUSTER_NAME \
        --principal-arn $AWS_USER_ARN  \
        --type STANDARD --user $AWS_USERNAME \
        --kubernetes-groups Viewers \
        --profile $awsprofile --region $AWS_REGION

        aws eks associate-access-policy --cluster-name $EKS_CLUSTER_NAME \
        --profile $awsprofile --region $AWS_REGION \
        --principal-arn $AWS_USER_ARN  --access-scope type=cluster \
        --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy
      
    - name: Update kube config
      run: |
        aws eks update-kubeconfig --region $AWS_REGION --profile $awsprofile --name $EKS_CLUSTER_NAME

        echo "Verifying the eks cluster"
        kubectl config current-context | grep $EKS_CLUSTER_NAME  | wc -l
        kubectl get namespace -A




        